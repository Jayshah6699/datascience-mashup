# -*- coding: utf-8 -*-
"""Car price prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10jKc0iKk3fTck00bH9Ycl7FknfsB9jnL

**Car Price Prediction**
"""

#IMPORT  LIBRARY FOR PROJECT
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import linear_model
from sklearn.linear_model import LinearRegression

#reading the dataset
dataset = pd.read_csv("/content/CarPrice_Assignment.csv")

#summary of dataset:205 rows 26 columns no null value
print(dataset.info())



dataset.head()

dataset['symboling'] = dataset['symboling'].astype('object')

dataset['CarName'][:30]

#extracting carnames
#str.split() by space
carnames = dataset['CarName'].apply(lambda x: x.split(" ")[0])
carnames[:30]

dataset['car_company'] = dataset['CarName'].apply(lambda x: x.split(" ")[0])

dataset['car_company'].astype('category').value_counts()

dataset = dataset.drop('CarName', axis =1)

dataset.describe()

dataset.info()

#symboling -3 least risky +2 more risky
dataset['symboling'].astype('category').value_counts()

#aspiration:An internal combustion (engine property) showing whether
#oxygen intake is  through standard atmospheric pressure
#through turbocharging
dataset['aspiration'].astype('category').value_counts()

#drive wheel: front wheel rear wheel four wheel
dataset['drivewheel'].astype('category').value_counts()

#wheelbase
sns.displot(dataset['wheelbase'])
plt.show()

#curbweight: weight without any baggage or person
sns.displot(dataset['curbweight'])
plt.show()

#stroke: volume of engine
sns.displot(dataset['stroke'])
plt.show()

#compression ratio: ratio of volume of compression chamber
# at largest capacity to least capacity
sns.displot(dataset['compressionratio'])
plt.show()

#target variable: price
sns.displot(dataset['price'])
plt.show()

#data exploration
dataset_numeric = dataset.select_dtypes(include=['float64','int'])
dataset_numeric.head()

plt.figure(figsize=(20,10))
sns.pairplot(dataset_numeric)
plt.show()

#correlation matrix
cor = dataset_numeric.corr()
cor

#plotting correlation on heatmap
plt.figure(figsize=(16,8))
sns.heatmap(cor,cmap='YlGnBu',annot=True)
plt.show()

"""Heatmap tell us various things on correlation

1.price is highly (positively) correlated with wheelbase,carlength,carwidth,curbweight,enginesize,horsepower.

2.price is negatively correlated with citympg,highwaympg.

Data Preperation
"""

y = dataset['price']
X = dataset[ ['symboling','aspiration','doornumber','carbody','fueltype',
              'drivewheel','enginelocation','wheelbase','carlength',
              'carwidth','carheight','curbweight','enginetype','cylindernumber',
              'enginesize','fuelsystem','boreratio','stroke','compressionratio','horsepower',
              'peakrpm','citympg','highwaympg','car_company']]

#crate dummy variable
dataset_categorical = X.select_dtypes(include=['object'])
dataset_categorical.head()

dataset_dummies = pd.get_dummies(dataset_categorical,drop_first=True)
dataset_dummies.head()

#drop categorical variable
X =X.drop(list(dataset_categorical.columns),axis=1)

#concat dummy variabl with x
X =pd.concat([X,dataset_dummies], axis=1)



#Scaling the features
from sklearn.preprocessing import scale
#storing column_names in cols, since columns_names are lost
#after scaling
cols = X.columns
X = pd.DataFrame(scale(X))
X.columns =  cols
X.columns

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    train_size = 0.7,
                                                    test_size = 0.3,
                                                    random_state=100)

model = LinearRegression()
model.fit(X_train,y_train)

print(model.coef_)
print(model.intercept_)

y_pred = model.predict(X_test)
from sklearn.metrics import r2_score
print(r2_score(y_true=y_test,y_pred=y_pred))

"""We are getting 82% accuracy of r squared with all variables.
let's see how we use less features and get good accuracy

Model building using recursive features elimination to select features
"""

#RFE with 15 features
from sklearn.feature_selection import RFE
model = LinearRegression()
rfe_15 = RFE(model, 15)
rfe_15.fit(X_train,y_train)
print(rfe_15.support_)
print(rfe_15.ranking_)

#making prediction rfe model
y_pred =rfe_15.predict(X_test)
print(r2_score(y_test,y_pred))

#RFE with 6 features
from sklearn.feature_selection import RFE
model = LinearRegression()
rfe_6 = RFE(model, 6)
rfe_6.fit(X_train,y_train)
print(rfe_6.support_)
print(rfe_6.ranking_)

#making prediction rfe model
y_pred =rfe_6.predict(X_test)
print(r2_score(y_test,y_pred))

"""RFE with 15 features we get 88.9% r squared value whereas RFE with 6 features we get 89.74 r squared value.

So we are choosing rfe_6

"""